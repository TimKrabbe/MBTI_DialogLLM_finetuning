{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8c1dadb",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0293ae",
   "metadata": {},
   "source": [
    "# Packages\n",
    "\n",
    "datasets function might need to be revised, in case this is part of the script uploaded to cluster\n",
    "--> depends where the data is coming from, might be able to upload somewhere. If uploaded to HF for example, we can't use load_from_disk, but have to use load_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890314d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft import PeftModel, PeftConfig, LoraConfig\n",
    "from huggingface_hub import login, upload_folder\n",
    "\n",
    "# local login\n",
    "#login(token = \"HF-Token\", add_to_git_credential=True)\n",
    "\n",
    "#colab login\n",
    "login(token = \"HF-Token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e3de07",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaefdd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full dataset local\n",
    "#raw_datasets = load_from_disk(\"../data/mbti_dict_ver2\")\n",
    "\n",
    "# colab, get dataset from HF\n",
    "raw_datasets = load_dataset(\"DrinkIcedT/mbti\")\n",
    "\n",
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_validation_dataset = raw_datasets[\"validation\"]\n",
    "raw_test_dataset = raw_datasets[\"test\"]\n",
    "\n",
    "\n",
    "# for running local test, variable dataset size\n",
    "#raw_datasets = raw_datasets.filter(lambda _, indices: indices % 25 == 0, with_indices=True)\n",
    "#raw_datasets.save_to_disk(\"..\\data\\smol\")\n",
    "\n",
    "# small dataset local\n",
    "#raw_datasets = load_from_disk(\"..\\data\\smol\")\n",
    "\n",
    "\n",
    "\n",
    "# small datasets split\n",
    "# raw_train_dataset = raw_datasets[\"train\"]\n",
    "# raw_validation_dataset = raw_datasets[\"validation\"]\n",
    "# raw_test_dataset = raw_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199a5b4a",
   "metadata": {},
   "source": [
    "# Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f848c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_checkpoint = None\n",
    "# qwen_checkpoint = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "# qwen_tokenizer = AutoTokenizer.from_pretrained(qwen_checkpoint)\n",
    "\n",
    "\n",
    "llama_checkpoint = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(llama_checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfe332c",
   "metadata": {},
   "source": [
    "# Chat-Templates and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e1d676",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = raw_train_dataset[\"post\"][0]\n",
    "print(test_sentence)\n",
    "\n",
    "tokenized_sentence = llama_tokenizer(test_sentence)\n",
    "print(tokenized_sentence)\n",
    "\n",
    "print(llama_tokenizer.convert_ids_to_tokens(tokenized_sentence[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef9c539",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_chatml(data):\n",
    "    \n",
    "    prompt = f\"Your personality Type is {data[\"label\"]}. What is on your mind?\"\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "            {\"role\": \"assistant\", \"content\": data[\"post\"]}\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7204fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds = raw_datasets.map(convert_to_chatml)\n",
    "print(ds[\"train\"][0][\"messages\"])\n",
    "\n",
    "messages = ds[\"train\"][0][\"messages\"]\n",
    "\n",
    "text = llama_tokenizer.applychat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True\n",
    ")\n",
    "\n",
    "model_inputs = llama_tokenizer([text]), return_tensors = \"pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b835b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#peft config\n",
    "\n",
    "rank_dimension = 8\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.05\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=rank_dimension,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias = \"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2a9118",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_chatml(example, checkpoint):\n",
    "    mbti_type = raw_datasets[\"train\"].features[\"label\"].int2str(example[\"label\"])\n",
    "\n",
    "    if checkpoint == qwen_checkpoint:\n",
    "        prompt = f\"Your personality Type is {mbti_type}. What is on your mind?\"\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "                {\"role\": \"assistant\", \"content\": example[\"post\"]}\n",
    "            ]\n",
    "        }\n",
    "    elif checkpoint == llama_checkpoint:\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": f\"You are a person with personality type {mbti_type} who responds accordingly!\"},\n",
    "                {\"role\": \"user\", \"content\": \"What is on your mind?\"},\n",
    "                {\"role\": \"assistant\", \"content\": example[\"post\"]}\n",
    "            ]\n",
    "        }\n",
    "\n",
    "ds = raw_datasets.map(\n",
    "    convert_to_chatml, \n",
    "    fn_kwargs={\"checkpoint\": llama_checkpoint}\n",
    ")\n",
    "\n",
    "# print(ds[\"train\"][0][\"messages\"])\n",
    "\n",
    "# messages = ds[\"train\"][0][\"messages\"]\n",
    "\n",
    "# text = qwen_tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     tokenize = False,\n",
    "#     add_generation_prompt = True\n",
    "# )\n",
    "\n",
    "#model_inputs = qwen_tokenizer([text], return_tensors = \"pt\")\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir = \"./mbti_test_output\",\n",
    "    max_steps = 100,\n",
    "    max_length = 512, #oder 256\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate = 5e-5,\n",
    "    logging_steps = 10,\n",
    "    save_steps=100,\n",
    "    eval_strategy = \"steps\",\n",
    "    eval_steps = 50,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=llama_checkpoint,\n",
    "    args=training_args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"validation\"],\n",
    "    dataset_text_field = \"messages\",\n",
    "    processing_class=llama_tokenizer,\n",
    "    peft_config=lora_config,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc49aa1c",
   "metadata": {},
   "source": [
    "### Watch out for:\n",
    "    Watch for these warning signs during training:\n",
    "\n",
    "        Validation loss increasing while training loss decreases (overfitting)\n",
    "        No significant improvement in loss values (underfitting)\n",
    "        Extremely low loss values (potential memorization)\n",
    "        Inconsistent output formatting (template learning issues)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
