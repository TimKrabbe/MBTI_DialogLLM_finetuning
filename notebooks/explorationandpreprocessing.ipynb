{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "432f4391",
   "metadata": {},
   "source": [
    "## Libraries and data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4345749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from cleantext import clean #pip install clean-text\n",
    "import re\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, ClassLabel, Value, Features,DatasetDict\n",
    "from huggingface_hub import upload_folder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343ea3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"datasnaek/mbti-type\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbd20c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/Tim/.cache/kagglehub/datasets/datasnaek/mbti-type/versions/1/mbti_1.csv\")\n",
    "\n",
    "df.head()\n",
    "length_df = len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563e9e3c",
   "metadata": {},
   "source": [
    "# Target variable distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8f5296",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_exp = pd.DataFrame(columns = [\"type\", \"counts\", \"percentage\"])\n",
    "target_exp[\"type\"] = df[\"type\"].unique()\n",
    "target_exp = df[\"type\"].value_counts().reset_index()\n",
    "target_exp[\"percentage\"] = round(target_exp[\"count\"] / target_exp[\"count\"].sum() * 100, ndigits = 3)\n",
    "for i in range(4):\n",
    "    target_exp[f\"axis {i+1} (I/E)\"] = target_exp[\"type\"].str[i]\n",
    "\n",
    "target_exp.loc[[0],[\"approx. general pop. freq.\"]] = 7\n",
    "target_exp.loc[[1],[\"approx. general pop. freq.\"]] = 2\n",
    "target_exp.loc[[2],[\"approx. general pop. freq.\"]] = 4\n",
    "target_exp.loc[[3],[\"approx. general pop. freq.\"]] = 3\n",
    "target_exp.loc[[4],[\"approx. general pop. freq.\"]] = 3.5\n",
    "target_exp.loc[[5],[\"approx. general pop. freq.\"]] = 7\n",
    "target_exp.loc[[6],[\"approx. general pop. freq.\"]] = 5\n",
    "target_exp.loc[[7],[\"approx. general pop. freq.\"]] = 7\n",
    "target_exp.loc[[8],[\"approx. general pop. freq.\"]] = 3.5\n",
    "target_exp.loc[[9],[\"approx. general pop. freq.\"]] = 12.5\n",
    "target_exp.loc[[10],[\"approx. general pop. freq.\"]] = 3.5\n",
    "target_exp.loc[[11],[\"approx. general pop. freq.\"]] = 11.5\n",
    "target_exp.loc[[12],[\"approx. general pop. freq.\"]] = 4.5\n",
    "target_exp.loc[[13],[\"approx. general pop. freq.\"]] = 6.5\n",
    "target_exp.loc[[14],[\"approx. general pop. freq.\"]] = 11\n",
    "target_exp.loc[[15],[\"approx. general pop. freq.\"]] = 10\n",
    "\n",
    "\n",
    "target_exp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c4ca80",
   "metadata": {},
   "source": [
    "# Plotting Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5981978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = target_exp.melt(\n",
    "    id_vars = \"type\",\n",
    "    value_vars= [\"percentage\", \"approx. general pop. freq.\"],\n",
    "    var_name = \"Group\",\n",
    "    value_name = \"Percentage\"\n",
    ")\n",
    "\n",
    "plot_data[\"Group\"] = plot_data[\"Group\"].replace({\n",
    "    \"percentage\": \"Sample\",\n",
    "    \"approx. general pop. freq.\": \"General Population (approx.)\"\n",
    "})\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10,6))\n",
    "\n",
    "\n",
    "sns.barplot(data = plot_data, x = \"type\", y = \"Percentage\", hue = \"Group\", alpha = 0.6)\n",
    "ax1.set_ylabel(\"Percentage\")\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax2.set_ylabel(\"Count (Sample only)\")\n",
    "total_n = target_exp[\"count\"].sum()\n",
    "max_perc = max(target_exp[\"percentage\"].max(), target_exp[\"approx. general pop. freq.\"].max())\n",
    "\n",
    "\n",
    "ax1.set_ylim(0, max_perc * 1.1)  \n",
    "ax2.set_ylim(0, (max_perc * 1.1) * total_n / 100)\n",
    "ax1.set_xlabel(\"MBTI Type\")\n",
    "plt.legend()\n",
    "plt.title(\"MBTI Distribution Dataset vs General Population\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8edbfb",
   "metadata": {},
   "source": [
    "# Data Cleaning and Splitting\n",
    "- mask urls\n",
    "- mask emails\n",
    "- mask phone numbers\n",
    "- mask ip addresses\n",
    "- mask file paths\n",
    "- mask mbti type mentions\n",
    "- drop rows with less than 30 characters\n",
    "- drop rows that are only numeric\n",
    "- drop duplicates\n",
    "\n",
    "## Splitting\n",
    "Each observation of \"posts\" in the dataset represents a collection of forum posts by a user. Since these posts are usually not related and there is no \"dialogue\"-like structure to them, splitting them allows for generating more data points (\"50-times more\") without any/much loss of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cadde35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning function\n",
    "def clean_data(text):\n",
    "    return clean(text,\n",
    "           fix_unicode = True,\n",
    "           no_urls = True,\n",
    "           no_emails = True,\n",
    "           no_file_paths = True,\n",
    "           no_phone_numbers = True,\n",
    "           no_ip_addresses = True\n",
    "           #no_emoji=True\n",
    "           )\n",
    "\n",
    "# splitting function\n",
    "def split_strings(data, col_to_split, separator):\n",
    "    dfcopy = data.copy().astype(str)\n",
    "    dfcopy[col_to_split] = dfcopy[col_to_split].str.split(pat = separator, regex=False)\n",
    "    return dfcopy.explode(col_to_split).reset_index(drop = True)\n",
    "\n",
    "\n",
    "# replace mbti mentions\n",
    "def replace_mbti(text):\n",
    "    pattern = r'\\b(infj|infp|intj|intp|isfj|isfp|istj|istp|enfj|enfp|entj|entp|esfj|esfp|estj|estp)\\b'\n",
    "    return re.sub(pattern, \"<mbti>\", text, flags=re.IGNORECASE)\n",
    "\n",
    "def preprocessing_pipe(df, col_to_clean, col_cleaned, separator):\n",
    "    # cleaning\n",
    "    df[col_cleaned] = df[col_to_clean].apply(clean_data)\n",
    "\n",
    "    #mbti type removal\n",
    "    df[col_cleaned] = df[col_cleaned].apply(replace_mbti)\n",
    "\n",
    "    #splitting\n",
    "    df = split_strings(data = df, col_to_split= col_cleaned, separator= separator)\n",
    "\n",
    "    #drop short posts\n",
    "    df = df.loc[df[col_cleaned].str.len() > 30].reset_index(drop = True)\n",
    "\n",
    "    # drop posts that are just numbers\n",
    "    df = df[~df[col_cleaned].str.isnumeric()]\n",
    "\n",
    "    # drop possible duplicates\n",
    "    df = df.drop_duplicates(subset=[col_cleaned])\n",
    "    df = df.drop(columns=[col_to_clean])\n",
    "    return df\n",
    "\n",
    "\n",
    "df_clean = preprocessing_pipe(df, \"posts\", \"posts_cleaned\", \"|||\")\n",
    "length_cleaned = len(df_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b1b94d",
   "metadata": {},
   "source": [
    "# Comparison cleaned vs not cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b7a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length of initial dataset: {length_df} \\n Length of cleaned dataset: {length_cleaned}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07578338",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_exp_clean = pd.DataFrame(columns = [\"type\", \"counts\", \"percentage\"])\n",
    "target_exp_clean[\"type\"] = df_clean[\"type\"].unique()\n",
    "target_exp_clean = df_clean[\"type\"].value_counts().reset_index()\n",
    "target_exp_clean[\"percentage\"] = round(target_exp_clean[\"count\"] / target_exp_clean[\"count\"].sum() * 100, ndigits = 3)\n",
    "for i in range(4):\n",
    "    target_exp_clean[f\"axis {i+1} (I/E)\"] = target_exp_clean[\"type\"].str[i]\n",
    "\n",
    "target_exp_clean.loc[[0],[\"approx. general pop. freq.\"]] = 7\n",
    "target_exp_clean.loc[[1],[\"approx. general pop. freq.\"]] = 2\n",
    "target_exp_clean.loc[[2],[\"approx. general pop. freq.\"]] = 4\n",
    "target_exp_clean.loc[[3],[\"approx. general pop. freq.\"]] = 3\n",
    "target_exp_clean.loc[[4],[\"approx. general pop. freq.\"]] = 3.5\n",
    "target_exp_clean.loc[[5],[\"approx. general pop. freq.\"]] = 7\n",
    "target_exp_clean.loc[[6],[\"approx. general pop. freq.\"]] = 5\n",
    "target_exp_clean.loc[[7],[\"approx. general pop. freq.\"]] = 7\n",
    "target_exp_clean.loc[[8],[\"approx. general pop. freq.\"]] = 3.5\n",
    "target_exp_clean.loc[[9],[\"approx. general pop. freq.\"]] = 12.5\n",
    "target_exp_clean.loc[[10],[\"approx. general pop. freq.\"]] = 3.5\n",
    "target_exp_clean.loc[[11],[\"approx. general pop. freq.\"]] = 11.5\n",
    "target_exp_clean.loc[[12],[\"approx. general pop. freq.\"]] = 4.5\n",
    "target_exp_clean.loc[[13],[\"approx. general pop. freq.\"]] = 6.5\n",
    "target_exp_clean.loc[[14],[\"approx. general pop. freq.\"]] = 11\n",
    "target_exp_clean.loc[[15],[\"approx. general pop. freq.\"]] = 10\n",
    "\n",
    "\n",
    "target_exp_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77d19df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plot_data_clean = target_exp_clean.melt(\n",
    "    id_vars = \"type\",\n",
    "    value_vars= [\"percentage\", \"approx. general pop. freq.\"],\n",
    "    var_name = \"Group\",\n",
    "    value_name = \"Percentage\"\n",
    ")\n",
    "\n",
    "plot_data_clean[\"Group\"] = plot_data_clean[\"Group\"].replace({\n",
    "    \"percentage\": \"Sample\",\n",
    "    \"approx. general pop. freq.\": \"General Population (approx.)\"\n",
    "})\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10,6))\n",
    "\n",
    "\n",
    "sns.barplot(data = plot_data_clean, x = \"type\", y = \"Percentage\", hue = \"Group\", alpha = 0.6)\n",
    "ax1.set_ylabel(\"Percentage\")\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax2.set_ylabel(\"Count (Sample only)\")\n",
    "total_n = target_exp_clean[\"count\"].sum()\n",
    "max_perc = max(target_exp_clean[\"percentage\"].max(), target_exp_clean[\"approx. general pop. freq.\"].max())\n",
    "\n",
    "\n",
    "ax1.set_ylim(0, max_perc * 1.1)  \n",
    "ax2.set_ylim(0, (max_perc * 1.1) * total_n / 100)\n",
    "ax1.set_xlabel(\"MBTI Type\")\n",
    "plt.legend()\n",
    "plt.title(\"MBTI Distribution in Cleaned Dataset vs General Population\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ec462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Erstelle das Grid (1 Zeile, 2 Spalten)\n",
    "fig, (axes_left, axes_right) = plt.subplots(2, 1, figsize=(18, 7))\n",
    "\n",
    "# --- DEIN PLOT (kommt auf die linke Seite: axes_left) ---\n",
    "# Wichtig: ax = axes_left zuweisen!\n",
    "sns.barplot(data=plot_data_clean, x=\"type\", y=\"Percentage\", hue=\"Group\", alpha=0.6, ax=axes_left)\n",
    "\n",
    "axes_left.set_ylabel(\"Percentage\")\n",
    "axes_left.set_xlabel(\"MBTI Type\")\n",
    "\n",
    "# Jetzt das twinx basierend auf axes_left erstellen\n",
    "ax2 = axes_left.twinx() \n",
    "ax2.set_ylabel(\"Count (Sample only)\")\n",
    "\n",
    "# Skalierung (Deine Logik Ã¼bernommen)\n",
    "total_n = target_exp[\"count\"].sum()\n",
    "max_perc = max(target_exp[\"percentage\"].max(), target_exp[\"approx. general pop. freq.\"].max())\n",
    "\n",
    "axes_left.set_ylim(0, max_perc * 1.1)\n",
    "ax2.set_ylim(0, (max_perc * 1.1) * total_n / 100)\n",
    "\n",
    "axes_left.set_title(\"MBTI Distribution vs General Population\")\n",
    "\n",
    "# --- DER ZWEITE PLOT (kommt auf die rechte Seite: axes_right) ---\n",
    "sns.barplot(data = plot_data_clean, x = \"type\", y = \"Percentage\", hue = \"Group\", alpha = 0.6,  ax = axes_right)\n",
    "ax1.set_ylabel(\"Percentage\")\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax2.set_ylabel(\"Count (Sample only)\")\n",
    "total_n = target_exp_clean[\"count\"].sum()\n",
    "max_perc = max(target_exp_clean[\"percentage\"].max(), target_exp_clean[\"approx. general pop. freq.\"].max())\n",
    "\n",
    "\n",
    "ax1.set_ylim(0, max_perc * 1.1)  \n",
    "ax2.set_ylim(0, (max_perc * 1.1) * total_n / 100)\n",
    "ax1.set_xlabel(\"MBTI Type\")\n",
    "plt.legend()\n",
    "plt.title(\"MBTI Distribution in Cleaned Dataset vs General Population\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86812944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05246962",
   "metadata": {},
   "outputs": [],
   "source": [
    "rseed = 42\n",
    "\n",
    "df_clean = df_clean.rename(columns={\"type\": \"label\", \"posts_cleaned\": \"post\"})\n",
    "df_hf = Dataset.from_pandas(df_clean, preserve_index=False)\n",
    "\n",
    "\n",
    "df_hf = df_hf.class_encode_column(\"label\")\n",
    "mbti_labels = [\"ENFJ\", \"ENFP\", \"ENTJ\", \"ENTP\", \"ESFJ\", \"ESFP\", \"ESTJ\", \"ESTP\", \n",
    "               \"INFJ\", \"INFP\", \"INTJ\", \"INTP\", \"ISFJ\", \"ISFP\", \"ISTJ\", \"ISTP\"]\n",
    "\n",
    "\n",
    "new_features = df_hf.features.copy()\n",
    "new_features[\"label\"] = ClassLabel(names=mbti_labels)\n",
    "\n",
    "df_hf = df_hf.cast(new_features)\n",
    "\n",
    "split = df_hf.train_test_split(test_size = 0.2, shuffle = True, stratify_by_column=\"label\", seed = rseed)\n",
    "train = split[\"train\"]\n",
    "df_temp = split[\"test\"]\n",
    "\n",
    "split2 = df_temp.train_test_split(test_size = 0.5, shuffle = True, stratify_by_column= \"label\", seed = rseed)\n",
    "val = split2[\"train\"]\n",
    "test = split2[\"test\"]\n",
    "\n",
    "df_final = DatasetDict({\n",
    "    \"train\": train,\n",
    "    \"test\": test,\n",
    "    \"validation\": val\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c011b05",
   "metadata": {},
   "source": [
    "# Save the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d592510",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_clean.to_csv(\"..\\data\\mbti_cleaned_whole.csv\", header = [\"label\", \"post\"], index = False)\n",
    "#df_final.save_to_disk(\"..\\data\\mbti_dict_ver2\")\n",
    "\n",
    "\n",
    "#upload to HF\n",
    "#upload_folder(folder_path=\"..\\data\\mbti_dict_ver2\", repo_id=\"DrinkIcedT/mbti\", repo_type=\"dataset\")\n",
    "# upload_folder(folder_path=\"..\\data\\smol\", repo_id=\"DrinkIcedT/smol_mbti\", repo_type=\"dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edaad5c",
   "metadata": {},
   "source": [
    "# Notes:\n",
    "- split before cleaning\n",
    "- drop rows under ??? words/characters\n",
    "    - drop rows that are only numbers\n",
    "\n",
    "After cleaning is done, plot distribution again"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
